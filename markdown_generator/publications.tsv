pub_date	title	author_list	venue	short_text	excerpt	citation	url_slug	paper_url
2021-05-08	Lipschitz Normalization for Self-Attention Layers with Application to Graph Neural Networks	<b>George Dasoulas</b>, Kevin Scaman, Aladin Virmaux	International Conference on Machine Learning (ICML)	We derive a theoretical analysis on the Lipschitz continuity of attention layers and we show that enforcing Lipschitz continuity by normalizing the attention scores can significantly improve the performance of deep attention models.	Attention based neural networks are state of the art in a large range of applications. However, their performance tends to degrade when the number of layers increases. In this work, we show that enforcing Lipschitz continuity by normalizing the attention scores can significantly improve the performance of deep attention models. First, we show that, for deep graph attention networks (GAT), gradient explosion appears during training, leading to poor performance of gradient-based training algorithms. To address this issue, we derive a theoretical analysis of the Lipschitz continuity of attention modules and introduce LipschitzNorm, a simple and parameter-free normalization for selfattention mechanisms that enforces the model to be Lipschitz continuous. We then apply LipschitzNorm to GAT and Graph Transformers and show that their performance is substantially improved in the deep setting (10 to 30 layers). More specifically, we show that a deep GAT model with LipschitzNorm achieves state of the art results for node label prediction tasks that exhibit long-range dependencies, while showing consistent improvements over their unnormalized counterparts in benchmark node classification tasks.	None	lipschitznorm	https://arxiv.org/pdf/2103.04886
2021-01-15	Learning Parametrised Graph Shift Operators	<b>George Dasoulas</b>, Johannes Lutzeyer, Michalis Vazirgiannis	International Conference on Learning Representations (ICLR)	We propose a parametrised graph shift operator (PGSO) to encode graph structure, providing a unified view of the most common GSOs, and improve GNN performance by incorporating the PGSO into the model training in an end-to-end manner.	In many domains data is currently represented as graphs and therefore, the graph representation of this data becomes increasingly important in machine learning. Network data is, implicitly or explicitly, always represented using a graph shift operator (GSO) with the most common choices being the adjacency, Laplacian matrices and their normalisations. In this paper, a novel parametrised GSO (PGSO) is proposed, where specific parameter values result in the most commonly used GSOs and message-passing operators in graph neural network (GNN) frameworks. The PGSO is suggested as a replacement of the standard GSOs that are used in state-of-the-art GNN architectures and the optimisation of the PGSO parameters is seamlessly included in the model training. It is proved that the PGSO has real eigenvalues and a set of real eigenvectors independent of the parameter values and spectral bounds on the PGSO are derived. PGSO parameters are shown to adapt to the sparsity of the graph structure in a study on stochastic blockmodel networks, where they are found to automatically replicate the GSO regularisation found in the literature. On several real-world datasets the accuracy of state-of-the-art GNN architectures is improved by the inclusion of the PGSO in both node- and graph-classification tasks.	None	pgso	https://arxiv.org/abs/2101.10050
2021-01-04	Ego-based Entropy Measures for Structural Representations on Graphs	<b>George Dasoulas</b>, Giannis Nikolentzos, Kevin Scaman, Aladin Virmaux, Michalis Vazirgiannis	IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	We investigate feature augmentation methods of graph neural networks using structural entropy measures.	In complex networks, nodes that share similar structural characteristics often exhibit similar roles (e.g type of users in a social network or the hierarchical position of employees in a company). In order to leverage this relationship, a growing literature proposed latent representations that identify structurally equivalent nodes. However, most of the existing methods require high time and space complexity. In this paper, we propose VNEstruct, a simple approach for generating low-dimensional structural node embeddings, that is both time efficient and robust to perturbations of the graph structure. The proposed approach focuses on the local neighborhood of each node and employs the Von Neumann entropy, an information-theoretic tool, to extract features that capture the neighborhood's topology. Moreover, on graph classification tasks, we suggest the utilization of the generated structural embeddings for the transformation of an attributed graph structure into a set of augmented node attributes. Empirically, we observe that the proposed approach exhibits robustness on structural role identification tasks and state-of-the-art performance on graph classification tasks, while maintaining very high computational speed.	None	vnestruct	https://arxiv.org/abs/2003.00553
2020-07-15	Coloring Graph Neural Networks for Node Disambiguation	<b>George Dasoulas</b>, Ludovic Dos Santos, Kevin Scaman, Aladin Virmaux	International Joint Conference on Artificial Intelligence (IJCAI)	We introduce a universal approximation scheme of continuous functions on graphs.	In this paper, we show that a simple coloring scheme can improve, both theoretically and empirically, the expressive power of Message Passing Neural Networks(MPNNs). More specifically, we introduce a graph neural network called Colored Local Iterative Procedure (CLIP) that uses colors to disambiguate identical node attributes, and show that this representation is a universal approximator of continuous functions on graphs with node attributes. Our method relies on separability , a key topological characteristic that allows to extend well-chosen neural networks into universal representations. Finally, we show experimentally that CLIP is capable of capturing structural characteristics that traditional MPNNs fail to distinguish,while being state-of-the-art on benchmark graph classification datasets.	None	clip_paper	https://www.ijcai.org/Proceedings/2020/0294.pdf
2020-04-16	Hcore-Init: Neural Network Initialization based on Graph Degeneracy	Stratis Limnios, <b>George Dasoulas</b>, Dimitrios M. Thilikos, Michalis Vazirgiannis	International Conference on Pattern Recognition (ICPR)	We propose a graph-based initialization of neural networks extending graph degeneracy observations.	Neural networks are the pinnacle of Artificial Intelligence, as in recent years we witnessed many novel architectures, learning and optimization techniques for deep learning. Capitalizing on the fact that neural networks inherently constitute multipartite graphs among neuron layers, we aim to analyze directly their structure to extract meaningful information that can improve the learning process. To our knowledge graph mining techniques for enhancing learning in neural networks have not been thoroughly investigated. In this paper we propose an adapted version of the k-core structure for the complete weighted multipartite graph extracted from a deep learning architecture. As a multipartite graph is a combination of bipartite graphs, that are in turn the incidence graphs of hypergraphs, we design k-hypercore decomposition, the hypergraph analogue of k-core degeneracy. We applied k-hypercore to several neural network architectures, more specifically to convolutional neural networks and multilayer perceptrons for image recognition tasks after a very short pretraining. Then we used the information provided by the hypercore numbers of the neurons to re-initialize the weights of the neural network, thus biasing the gradient optimization scheme. Extensive experiments proved that k-hypercore outperforms the state-of-the-art initialization methods.	None	hcore	https://arxiv.org/abs/2004.07636
2019-07-13	k-hop Graph Neural Networks	Giannis Nikolentzos, <b>George Dasoulas</b>, Michalis Vazirgiannis	Neural Networks Journal, Volume 130	We iteratively extend the aggregation operator of graph neural networks to increase their receptive field. 	Graph neural networks (GNNs) have emerged recently as a powerful architecture for learning node and graph representations. Standard GNNs have the same expressive power as the Weisfeiler–Lehman test of graph isomorphism in terms of distinguishing non-isomorphic graphs. However, it was recently shown that this test cannot identify fundamental graph properties such as connectivity and triangle freeness. We show that GNNs also suffer from the same limitation. To address this limitation, we propose a more expressive architecture, k-hop GNNs, which updates a node’s representation by aggregating information not only from its direct neighbors, but from its k-hop neighborhood. We show that the proposed architecture can identify fundamental graph properties. We evaluate the proposed architecture on standard node classification and graph classification datasets. Our experimental evaluation confirms our theoretical findings since the proposed model achieves performance better or comparable to standard GNNs and to state-of-the-art algorithms.	None	khop	https://arxiv.org/abs/1907.06051
